{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "computational-experience",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red; text-align:center\">METHODOLOGY - Code</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-franchise",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "\n",
    "from unidecode import unidecode\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "import networkx.algorithms.community as nxcom\n",
    "from graphviz import Digraph, Graph\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-light",
   "metadata": {},
   "source": [
    "## Functions declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE THE DATASET\n",
    "\n",
    "# list of frequent and isignificant hashtags to remove\n",
    "stopwords=[\"meme\",\"memes\",\"memeita\", \"repost\", \"photooftheday\", \"picoftheday\", \"moodoftheday\", \"webstagram\", \"tagstagramers\", \"bestoftheday\",\n",
    "           \"photodaily\", \"tagsta\", \"igers\",  \"ootd\", \"shot\", \"shots\", \"pics\", \"instangram\", \"youtuber\", \"youtube\", \"boysofinstagram\", \"daily\",\n",
    "           \"goodlookingguys\", \"onthisday\", \"post\", \"tagga\", \"taggaituoiamici\", \"picby\",\n",
    "           \"follow\", \"follower\", \"followers\", \"followme\", \"followher\", \"alwaysfollowback\", \"pleasefollow\", \"followall\", \"followforfollow\",\n",
    "           \"followbackalways\", \"follow4followback\", \"teamfollowback\", \"pleasefollowme\", \"ifollow\", \"followbackteam\", \"follow4follow\",\n",
    "           \"followback\", \"followerforfollower\", \"followerforfollowers\", \"followersforfollower\", \"followersforfollowers\", \"followtrick\",\n",
    "           \"followalways\",\n",
    "           \"like\", \"likes\", \"likeforlike\", \"likeback\", \"likeforfollow\", \"likeforfollows\", \"likeforfollower\", \"likeforfollowers\",\n",
    "           \"likesforfollow\", \"likesforfollower\", \"likesforfollowers\", \"likeforlikeback\", \"likeforlikes\", \"likesforlikes\", \"tagsforlikes\",\n",
    "           \"tagforlike\", \"tagsforlike\", \"like4like\", \"likes4likes\", \"like4likes\", \"likes4like\", \"unlimlikes\",\n",
    "           \"instagram\", \"instagood\", \"instadaily\", \"instabeauty\", \"instacool\", \"instamood\", \"instalike\", \"instago\", \"instagramer\", \"instalove\",\n",
    "           \"instagramers\", \"instalife\", \"instapic\", \"instapics\", \"instavideo\", \"instastories\", \"instaglam\", \"instasaturday\", \"instanlike\",\n",
    "           \"instaphotography\", \"instadailypic\", \"instafamous\", \"instamoment\", \"instahome\", \"unforgettableinstagrammer\", \"instaphoto\",\n",
    "           \"instagrammer\", \"instagrammers\", \"instatag\", \"shout\", \"shoutout\", \"shoutout4shoutout\", \"shoutout4shoutouts\", \"shoutouter\", \n",
    "           \"shoutouters\", \"shoutoutforshoutout\", \"shoutoutforshoutouts\", \"shoutouts\", \"shoutouts4shoutout\", \"shoutouts4shoutouts\", \n",
    "           \"shoutoutsforshoutout\", \"shoutoutsforshoutouts\",\n",
    "           \"instalive\", \"toptags\" , \"ilove\", \"lovelife\", \n",
    "           \"instablog\", \"instaboy\", \"instagirl\", \"insta\", \"instaphotos\", \"instavideos\", \"instacolors\", \"instacolor\",\n",
    "           \"cool\", \"goodmorning\", \"iphoneonly\", \"lifestyle\", \"tumblr\", \"simply\", \"thanks\", \"thankyou\", \"youandme\", \"meandyou\", \"loveu\",\n",
    "           \"loveyou\", \"beauty\", \"mylove\", \"ioete\", \"forever\", \"cute\", \"beautifulday\", \"best\", \"mood\", \"smile\",\n",
    "           \"video\", \"photo\", \"happy\", \"happiness\", \"happyness\", \"photography\", \"beautiful\", \"life\", \"love\", \"amore\", \"amazing\", \"grazie\"]\n",
    "\n",
    "\n",
    "# Remove posts with expected reactions equal to 0\n",
    "def remove_posts_followers_zero(df_influencers):\n",
    "    \n",
    "    return df_influencers[df_influencers[\"followers\"]!=0]\n",
    "\n",
    "# Extract hashtags from the text in the description of a post\n",
    "# Input: DataFrame of all posts\n",
    "# Ouput: the same DataFrame with the \"list_hashtags\" column in addition\n",
    "def words_finder(df, field=\"description\"):\n",
    "    \n",
    "    lists_hashtags=[]\n",
    "    regex = \"#(\\w+)\"\n",
    "    \n",
    "    for text in df[field].to_list():\n",
    "     \n",
    "        lst2=[]\n",
    "\n",
    "        lst = re.findall(regex, str(text))\n",
    "\n",
    "        for s in lst:\n",
    "            lst2.append(unidecode_word(s))\n",
    "            \n",
    "        lst2=filter_words(lst2)\n",
    "            \n",
    "        lists_hashtags.append(lst2)\n",
    "        \n",
    "    df[\"list_hashtags\"]=lists_hashtags\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Unidecode words\n",
    "# Input: string of the word\n",
    "# Output: string of the unidecoded word\n",
    "def unidecode_word(text):    \n",
    "    \n",
    "    return deEmojify(unidecode(text))\n",
    "\n",
    "\n",
    "# Remove emojis in words\n",
    "# Input: string of the word\n",
    "# Output: string of the word without emojis\n",
    "def deEmojify(text):\n",
    "    \n",
    "    regrex_pattern = re.compile(pattern = \n",
    "        (\"[\\u263a-\\U0001f645]\"), flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "\n",
    "# Remove stopwords (frequent hashtags) from the list of hashtags\n",
    "# Input: list of hashtags\n",
    "# Ouput: filtered list of hashtags\n",
    "def filter_words(lst):\n",
    "    \n",
    "    lst=[hashtag for hashtag in lst if hashtag not in stopwords] \n",
    "    \n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-roommate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND EXPECTED REACTIONS FOR EACH POST PER INFLUENCER\n",
    "\n",
    "# Find expected reactions for each post for the current influencer\n",
    "# Input: DataFrame of all the posts of all influencers\n",
    "# Output: the same DataFrame, but with the \"expected\" column in addition, computed for each influencer\n",
    "def expected_reactions_per_infl(df):\n",
    "    \n",
    "    influencers=df[\"accountID\"].drop_duplicates().to_list()\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    for infl in influencers:\n",
    "        \n",
    "        print(\"Processing:\", infl)\n",
    "        tmp_df=df[df[\"accountID\"]==infl]\n",
    "        tmp_df=tmp_df.sort_values(by=[\"date\"])\n",
    "        \n",
    "        df_tmp = find_expected(tmp_df)\n",
    "        dfs.append(df_tmp)\n",
    "        \n",
    "    expected = pd.concat(dfs) \n",
    "    \n",
    "    return expected\n",
    "\n",
    "\n",
    "# Find expected reactions for each post for the current influencer\n",
    "# Input: DataFrame of the posts of an influencer\n",
    "# Output: the same DataFrame, but with the \"expected\" column in addition\n",
    "def find_expected(tmp_df):\n",
    "    \n",
    "    i=0\n",
    "    serie=tmp_df[\"reactions\"]\n",
    "    \n",
    "    tmp_df[\"expected\"]=0\n",
    "    \n",
    "    for post in tmp_df.itertuples():\n",
    "\n",
    "        # Expanding window to take the last 100 posts or, if there are fewer, all of them\n",
    "        samples = serie[ i-100 if i -100 -1>0 else 0 : i]\n",
    "        \n",
    "        # If there are less than 10 posts by the current influencer, ignore them\n",
    "        if len(samples)<10:\n",
    "            i+=1\n",
    "            continue\n",
    "            \n",
    "        Q1 = np.quantile(samples, 0.25)\n",
    "        Q3 = np.quantile(samples, 0.75)\n",
    "        \n",
    "        # Take the subset of samples in the interval [Q1, Q3]\n",
    "        sub_samples=[s for s in samples if ((s>Q1) & (s<Q3))]\n",
    "        \n",
    "        expected=np.mean(sub_samples)\n",
    "\n",
    "        tmp_df.loc[tmp_df[\"postID\"]==post.postID,\"expected\"]=expected\n",
    "            \n",
    "        i+=1\n",
    "    \n",
    "    return tmp_df\n",
    "\n",
    "\n",
    "# Remove posts with expected reactions equal to 0\n",
    "def remove_posts_expected_zero(df_influencers):\n",
    "    \n",
    "    return df_influencers[df_influencers[\"expected\"]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND OUTLIERS FOR EACH INFLUENCER - ANOMALY DETECTION PHASE\n",
    "\n",
    "# Find anomalies (notable posts) for the influencers through the Boxplot Rule method\n",
    "# Input: DataFrame of all the posts of all influencers\n",
    "# Output: DataFrame of notable posts of all influencers\n",
    "def Boxplot_rule_per_infl(df):\n",
    "    \n",
    "    influencers=df[\"accountID\"].drop_duplicates().to_list()\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    for infl in influencers:\n",
    "        \n",
    "        print(\"Processing:\", infl)\n",
    "        tmp_df=df[df[\"accountID\"]==infl]\n",
    "        tmp_df=tmp_df.sort_values(by=[\"date\"])\n",
    "        \n",
    "        df_tmp = find_outliers(tmp_df)\n",
    "        dfs.append(df_tmp)\n",
    "        \n",
    "    anomalies = pd.concat(dfs) \n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "\n",
    "# Find anomalies (notable posts) for the current influencer through the Boxplot Rule method\n",
    "# Input: DataFrame of the posts of an influencer\n",
    "# Output: DataFrame of notable posts of the same influencer\n",
    "def find_outliers(tmp_df):\n",
    "    \n",
    "    i=0\n",
    "    serie=tmp_df[\"score\"]\n",
    "    mask = []\n",
    "    \n",
    "    for post in tmp_df.itertuples():\n",
    "        \n",
    "        # Expanding window to take the last 100 posts or, if there are fewer, all of them\n",
    "        samples = serie[ i-100 if i -100 -1>0 else 0 : i]\n",
    "        \n",
    "        # If there are less than 10 posts by the current influencer, ignore them\n",
    "        if len(samples)<10:\n",
    "            i+=1\n",
    "            mask.append(False)\n",
    "            continue\n",
    "            \n",
    "        # Boxplot Rule formula\n",
    "        Q1 = np.quantile(samples, 0.25)\n",
    "        Q3 = np.quantile(samples, 0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        upper_limit=Q3 + 2 * IQR\n",
    "        \n",
    "        if post.score > upper_limit:\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "            \n",
    "        i+=1\n",
    "            \n",
    "    anomaly_df = tmp_df[mask]\n",
    "    \n",
    "    return anomaly_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A GRAPH OF NOTABLE POSTS PER WEEK AND FIND COMMUNITIES IN EACH GRAPH - CLUSTERING PHASE\n",
    "\n",
    "# Create a graph for each week:\n",
    "#  - each node is a post\n",
    "#  - each edge represents at least an hashtag in common between the text in their descriptions\n",
    "# Input:  DataFrame of notable posts (obtained in the Anomaly Detection phase)\n",
    "# Output: Dictionary (key,value) -> key:   string \"year_week\"\n",
    "#                                          value: NetworkX Graph object\n",
    "def create_graphs (df_outliers_over_hashtags):\n",
    "\n",
    "    graphs_over_list=dict()\n",
    "    graphs_over_post=dict()\n",
    "    \n",
    "    for group_name, df_group in df_outliers_over_hashtags.groupby([\"year\",\"week\"]):\n",
    "        \n",
    "        G1 = nx.Graph()\n",
    "        G2 = nx.Graph()\n",
    "        tag_to_posts_list = defaultdict(set)\n",
    "        tag_to_posts_ids = defaultdict(set)\n",
    "        \n",
    "        for row_index, x in df_group.iterrows():\n",
    "            \n",
    "            week=str(int(x.week))\n",
    "            year=str(int(x.year))\n",
    "            key=str(year+\"_\"+week)\n",
    "            list_hashtags = (x.list_hashtags)\n",
    "            \n",
    "            post_list = \" \".join(list_hashtags)\n",
    "            post_id = x.postID\n",
    "            \n",
    "            if len(list_hashtags) > 0:\n",
    "                G1.add_node(post_list)\n",
    "                G2.add_node(post_id)\n",
    "                for tag in list_hashtags:\n",
    "                    tag_to_posts_list[tag].add(post_list)\n",
    "                    tag_to_posts_ids[tag].add(post_id)\n",
    "                    \n",
    "        for tag in tag_to_posts_list:\n",
    "            for post_ext1,post_ext2 in zip(tag_to_posts_list[tag],tag_to_posts_ids[tag]):\n",
    "                for post_int1,post_int2 in zip(tag_to_posts_list[tag],tag_to_posts_ids[tag]):\n",
    "                    if post_ext2 != post_int2:\n",
    "                        weight=distance(post_ext1, post_int1)\n",
    "                        G1.add_edge(post_ext1, post_int1, weight=weight)\n",
    "                        G2.add_edge(post_ext2, post_int2, weight=weight)\n",
    "     \n",
    "        graphs_over_list[key] = G1 # graph in which each node is the list of hashtags of the post\n",
    "        graphs_over_post[key] = G2 # graph in which each node is a post (this graph is \"parallel\" to the previous)\n",
    "            \n",
    "    return graphs_over_post\n",
    "\n",
    "\n",
    "# Weight-function to ompute weight for each edge\n",
    "# Input:  hashtags lists of the two posts linked by the edge for which to calculate the weight\n",
    "# Output: weight of the edge\n",
    "def distance(lst1, lst2):\n",
    "    \n",
    "    s1 = set(lst1)\n",
    "    s2 = set(lst2)\n",
    "    \n",
    "    if(s1==s2):\n",
    "        return 0\n",
    "    \n",
    "    if(len(s1.union(s2))==0):\n",
    "        return 1.0\n",
    "    \n",
    "    # Partial intersections are a measure of how similar the words not present in both hashtag lists are, \n",
    "    # and are calculated by considering the length of the common letter blocks between the words\n",
    "    partial_intersections=0\n",
    "    \n",
    "    for word1 in s1-s1.intersection(s2):\n",
    "        combinations=0\n",
    "        for word2 in s2-s1.intersection(s2):\n",
    "            match = SequenceMatcher(None, word1, word2).get_matching_blocks()\n",
    "            match_sizes_sum = sum([m.size for m in match if m.size>2])\n",
    "            if(match_sizes_sum>0):\n",
    "                partial_intersections=partial_intersections+float(match_sizes_sum/(len(word1)+len(word2)))\n",
    "                combinations=combinations+1\n",
    "        if(combinations>0):\n",
    "            partial_intersections=float(partial_intersections/combinations)\n",
    "      \n",
    "    # Jaccard Distance formula\n",
    "    return 1-float((len(s1.intersection(s2))+partial_intersections) / len(s1.union(s2)))\n",
    "        \n",
    "\n",
    "# Find Louvain communities for each weekly graph and draws them through NetworkX spring_layout\n",
    "# Input:  dictionary of weekly graphs, DataFrame of notable posts (obtained in the Anomaly Detection phase)\n",
    "# Output: dictionary of DataFrames of notable posts clustered in the corresponding communities (key,value) -> key:   string \"year_week\"\n",
    "#                                                                                                             value: DataFrame\n",
    "def draw_communities(graphs, df_outliers):\n",
    "    \n",
    "    communities_dict=dict()\n",
    "    \n",
    "    for key in sorted(graphs.keys()):\n",
    "        \n",
    "        year=int(key.split(\"_\")[0])\n",
    "        week=int(key.split(\"_\")[1])\n",
    "        \n",
    "        tot_posts=int(df_outliers[(df_outliers[\"year\"]==year)&(df_outliers[\"week\"]==week)][\"n_anomalies\"].drop_duplicates())\n",
    "    \n",
    "        G = graphs[key]\n",
    "        error=0\n",
    "        \n",
    "        try:\n",
    "            communities_louvain = community_louvain.best_partition(G)\n",
    "        except ZeroDivisionError:\n",
    "            error=1\n",
    "            \n",
    "        mod=-1\n",
    "\n",
    "        node_groups = []\n",
    "                \n",
    "        # Louvain algorithm for community detection\n",
    "        if(error!=1):\n",
    "            for node,com in sorted(communities_louvain.items(), key=lambda item: item[1]):\n",
    "                node_groups.append([])\n",
    "                node_groups[com].append(node)\n",
    "            \n",
    "            degree = dict(G.degree(weight=\"weight\"))\n",
    "            clustering_coeff=nx.clustering(G)\n",
    "            deg_sum = sum(degree.values())\n",
    "            if(deg_sum!=0):\n",
    "                mod=modularity(G, node_groups)\n",
    "            else:\n",
    "                mod=-1\n",
    "        \n",
    "        useful_com_2 = [] # communities containing at least 2 posts\n",
    "        for com in node_groups:\n",
    "            if(len(list(com))>1):\n",
    "                useful_com_2.append(list(com))\n",
    "                \n",
    "        # Selecting colors to draw the graphs and the communities  \n",
    "        nodes_color_map = []\n",
    "        edge_color_map = []\n",
    "        colors = plt.cm.get_cmap(\"tab20\", len(useful_com_2))\n",
    "        node_groups=sorted(node_groups, key=len, reverse=True)\n",
    "        \n",
    "        for node in G:\n",
    "            found=0\n",
    "            for i in range(len(node_groups)):\n",
    "                if node in node_groups[i]:\n",
    "                    if(len(node_groups[i])>1):\n",
    "                        nodes_color_map.append(colors(i))\n",
    "                        edge_color_map.append(colors(i))\n",
    "                        found=1\n",
    "                    break\n",
    "            if found==0:\n",
    "                nodes_color_map.append(\"white\")\n",
    "                edge_color_map.append(\"black\")\n",
    "              \n",
    "        # Drawing the graph and the communities\n",
    "        plt.figure(figsize=(12,7))   \n",
    "        pos = nx.spring_layout(G, k=0.2, iterations=50)\n",
    "        nx.draw(G, pos, node_color=nodes_color_map, edgecolors=edge_color_map, node_size=100, with_labels=False)\n",
    "        plt.show()\n",
    "        \n",
    "        # Building the final DataFrame\n",
    "        communities_df=df_outliers\n",
    "        communities_df=communities_df[(communities_df[\"year\"]==year) & (communities_df[\"week\"]==week)]\n",
    "        communities_df[\"community\"]=-1\n",
    "        communities_df[\"degree\"]=-1\n",
    "        communities_df[\"clustering_coeff\"]=-1\n",
    "        communities_df[\"modularity\"]=mod\n",
    "        if(len(node_groups)==1):\n",
    "            communities_df[\"n_communities\"]=0\n",
    "        else:\n",
    "            communities_df[\"n_communities\"]=len(node_groups)\n",
    "        communities_df[\"n_communities2\"]=len(useful_com_2)\n",
    "        \n",
    "        mean_degree=np.array(list(degree.values())).mean()\n",
    "        median_degree=np.median(list(degree.values()))\n",
    "        communities_df[\"mean_degree\"]=mean_degree\n",
    "        communities_df[\"median_degree\"]=median_degree\n",
    "        mean_clustering_coeff=np.array(list(clustering_coeff.values())).mean()\n",
    "        median_clustering_coeff=np.median(list(clustering_coeff.values()))\n",
    "        communities_df[\"mean_clustering_coeff\"]=mean_clustering_coeff\n",
    "        communities_df[\"median_clustering_coeff\"]=median_clustering_coeff\n",
    "        \n",
    "        # Printing the statistics for the current graph\n",
    "        print(\"Year: \"+str(year))\n",
    "        print(\"Week: \"+str(week))\n",
    "        print(\"Total number of posts of the week: \"+str(tot_posts))\n",
    "        print(\"Nodes: \"+str(len(G.nodes)))\n",
    "        print(\"Edges: \"+str(len(G.edges)))\n",
    "        print(\"Communities: \"+str(len(node_groups)))\n",
    "        print(\"Communities with at least 2 nodes (posts): \"+str(len(useful_com_2)))\n",
    "        print(\"Modularity: \"+str(mod))\n",
    "        print(\"Mean degree: \"+str(mean_degree))\n",
    "        print(\"Median degree: \"+str(median_degree))\n",
    "        print(\"Mean clustering coefficient: \"+str(mean_clustering_coeff))\n",
    "        print(\"Median clustering coefficient: \"+str(median_clustering_coeff))\n",
    "        \n",
    "        i=0\n",
    "        for community in useful_com_2:\n",
    "            for post in community:\n",
    "                communities_df.loc[communities_df[\"postID\"]==post, \"community\"]=i\n",
    "                communities_df.loc[communities_df[\"postID\"]==post, \"degree\"]=G.degree[post]\n",
    "                communities_df.loc[communities_df[\"postID\"]==post, \"clustering_coeff\"]=nx.clustering(G,post)\n",
    "            i=i+1\n",
    "           \n",
    "        communities_dict[key]=communities_df.sort_values(by=[\"community\"])\n",
    "    \n",
    "    return communities_dict\n",
    "\n",
    "\n",
    "# Alternative method to draw graphs and communities (nodes with different shapes per community)\n",
    "# Input: NetworkX graph\n",
    "# Output: draws the graph and the communities\n",
    "def draw_graph(G):\n",
    "    \n",
    "    G= G.copy()\n",
    "    print (\"Nodes all:\", len (G.nodes))\n",
    "    \n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    \n",
    "    print (\"Nodes with edges:\", len (G.nodes))\n",
    "    print (\"Edges:\", len (G.edges))\n",
    "    \n",
    "    partition = community_louvain.best_partition(G)\n",
    "    shapes = 'so^>v<dph8so^>v<dph8so^>v<dph8so^>v<dph8so^>v<dph8so^>v<dph8so^>v<dph8so^>v<dph8so^>v<dph8'\n",
    "\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.axis('off')\n",
    "\n",
    "    pos = nx.nx_pydot.graphviz_layout(G)\n",
    "    \n",
    "    # Color the nodes according to their partition\n",
    "    cmap = cm.get_cmap('jet')\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "    i=0\n",
    "    for node, color in partition.items():\n",
    "        nx.draw_networkx_nodes(G, pos, [node], node_size=100,\n",
    "                               node_color=[cmap(color/len(set(partition.values())) )],\n",
    "                               node_shape=shapes[color])\n",
    "        i+=1\n",
    "        \n",
    "    plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot wordclouds of the current week per community\n",
    "# Input: DataFrame of the notable posts of the week clustered in communities, a string in the form \"year_week\"\n",
    "# Output: plots the wordclouds\n",
    "def wordcloud_per_week(df_week, prefix):\n",
    "    \n",
    "    communities=df_week[df_week[\"community\"]!=-1][\"community\"].unique()\n",
    "    \n",
    "    for i,com in enumerate(communities):\n",
    "        \n",
    "        lists_words=df_week[df_week[\"community\"]==com][\"list_hashtags\"].to_list()\n",
    "        \n",
    "        words=[]\n",
    "        for lst in lists_words:\n",
    "            for w in lst:\n",
    "                words.append(w)\n",
    "            \n",
    "        print(f\"Community: {i}. Posts: {len(df_week[df_week['community']==com])}\")\n",
    "        print(\"Profiles:\", \", \".join(df_week[df_week[\"community\"]==com][\"name\"].drop_duplicates()))\n",
    "        frequencies=dict()\n",
    "        \n",
    "        for w in set(words):\n",
    "            f=words.count(w)\n",
    "            frequencies[w]=f\n",
    "            \n",
    "        # Wordclouds of the current week\n",
    "        if len(frequencies) > 0:\n",
    "            fig=plt.figure(figsize=(5,2.5))\n",
    "            wordcloud = WordCloud(background_color=None, mode=\"RGBA\", collocations=False, mask=None)\n",
    "            wordcloud.generate_from_frequencies(frequencies=frequencies)\n",
    "\n",
    "            plt.imshow(wordcloud, cmap=plt.cm.gray, interpolation=\"bilinear\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n",
    "            plt.savefig(f\"wordcloud_{prefix}_{i}.pdf\", dpi=1000)\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-length",
   "metadata": {},
   "source": [
    "## Dataset transformation\n",
    "\n",
    "* **Input**: a Pandas DataFrame (df_influencers) with columns:\n",
    "    * **postID**: id of the post;\n",
    "    * **AccountID**: id of the influencer account that published the post;\n",
    "    * **name**: name of the influencer account that published the post;\n",
    "    * **date**: exact publication date of the post;\n",
    "    * **year**: year of publication of the post;\n",
    "    * **week**: week of publication of of the post;\n",
    "    * **description**: text of the post;\n",
    "    * **reactions**: number of \"likes\" and \"comments\" effectively received by the post;\n",
    "    * **followers**: number of followers of the influencer at the time of the publication of the post.\n",
    "    \n",
    "    \n",
    "* **Output**: the same Pandas DataFrame with the following columns in addition:\n",
    "    * **list_hashtags**: the list of hashtags extracted from the text of the post;\n",
    "    * **expected**: number of \"expected\" reactions of the post;\n",
    "    * **score**: performance \"score\" of the post (effective reactions/expected reactions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove posts of influencers with zero followers at the publication date of the post\n",
    "df_influencers_purged=remove_posts_followers_zero(df_influencers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-remainder",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract list of hashtags from each post\n",
    "df_influencers_purged=words_finder(df_influencers_purged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-guyana",
   "metadata": {},
   "source": [
    "### Compute expected reactions for each post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-pressing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_expected=expected_reactions_per_infl(df_influencers_purged)\n",
    "df_expected=remove_posts_expected_zero(df_expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-radiation",
   "metadata": {},
   "source": [
    "### Compute performance score for each post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-hindu",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_expected[\"score\"]=df_expected[\"reactions\"]/df_expected[\"expected\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-douglas",
   "metadata": {},
   "source": [
    "## ANOMALY DETECTION\n",
    "Apply the Boxplot Rule method to extract **notable posts** for each influencer and save them in a csv file (\"anomalies.csv\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-locking",
   "metadata": {},
   "source": [
    "* **Output**: the same previous Pandas DataFrame with the following column in addition:\n",
    "    * **n_anomalies**: number of anomalies for each week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-poultry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_outliers=Boxplot_rule_per_infl(df_expected)\n",
    "df_outliers.to_csv(\"anomalies.csv\", index=False, header=True, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tojoin=df_outliers.groupby([\"year\",\"week\"]).agg(n_anomalies=pd.NamedAgg(column=\"postID\", aggfunc=\"nunique\"))\n",
    "df_outliers=df_outliers.join(df_tojoin, on=[\"year\",\"week\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-truck",
   "metadata": {},
   "source": [
    "## CLUSTERING (Graphs creation and Louvain Community Detection algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-orbit",
   "metadata": {},
   "source": [
    "* **Output**: the same previous Pandas DataFrame with the following columns in addition:\n",
    "    * **community**: identification number of the community to which the post belongs;\n",
    "    * **degree**: degree of the node (post) in the graph;\n",
    "    * **clustering_coeff**: clustering coefficient of the node (post) in the graph;\n",
    "    * **modularity**: modularity of the communities in the graph;\n",
    "    * **n_communities**: total number of communities in the graph;\n",
    "    * **n_communities2**: number of communities containing at least 2 nodes (posts) in the graph;\n",
    "    * **mean_degree**: mean of the degree of the nodes in the graph;\n",
    "    * **median_degree**: median of the degree of the nodes in the graph;\n",
    "    * **mean_clustering_coeff**: mean of the clustering coefficient of the nodes in the graph;\n",
    "    * **median_clustering_coeff**: median of the clustering coefficient of the nodes in the graph.\n",
    "    \n",
    "Here, the term \"graph\" refers to the graph of notable posts of each week.\n",
    "The ouput DataFrame is then saved in a csv file (\"communities.csv\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordering the dataset\n",
    "df_outliers=df_outliers.sort_values(by=[\"year\",\"week\"])\n",
    "\n",
    "# Get graphs\n",
    "graphs_post=create_graphs(df_outliers)\n",
    "\n",
    "# Get communities by Louvain algorithm\n",
    "df_communities=draw_communities(graphs_post, df_outliers)\n",
    "\n",
    "df_communities.to_csv(\"communities.csv\", index=False, header=True, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-victory",
   "metadata": {},
   "source": [
    "## WORDCLOUDS per community per week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a year and a week and plot the wordcloud for each community for the selected year and week\n",
    "key=\"2020_12\"\n",
    "df_week=df_communities[key]\n",
    "df_week=df_week[(df_week[\"year\"]==2020) & (df_week[\"week\"]==12)]\n",
    "\n",
    "wordcloud_per_week(df_week, prefix=key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
